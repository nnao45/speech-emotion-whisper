from typing import Dict, List, Tuple
import numpy as np
from rich.console import Console
from rich.table import Table
from rich.panel import Panel


class ResultDisplay:
    """Display emotion analysis results with rich formatting."""
    
    def __init__(self):
        self.console = Console()
        
        # Emotion label translation dictionary (English -> Japanese)
        self.emotion_translations = {
            "angry": "æ€’ã‚Š",
            "disgust": "å«Œæ‚ª", 
            "fearful": "ææ€–",
            "happy": "å–œã³",
            "neutral": "å¹³é™",
            "sad": "æ‚²ã—ã¿",
            "surprised": "é©šã"
        }
    
    def _translate_emotion(self, emotion: str) -> str:
        """
        Translate emotion label to Japanese.
        
        Args:
            emotion: English emotion label
            
        Returns:
            Japanese emotion label
        """
        return self.emotion_translations.get(emotion.lower(), emotion)
    
    def show_processing_start(self, file_path: str, mode: str):
        """Show processing start message."""
        panel = Panel(
            f"[bold blue]ğŸ¬ Processing:[/bold blue] {file_path}\n"
            f"[bold yellow]ğŸ“Š Mode:[/bold yellow] {mode}",
            title="[bold green]Speech Emotion Recognition[/bold green]",
            border_style="blue"
        )
        self.console.print(panel)
    
    def show_single_result(self, emotion_scores: Dict[str, float], detailed: bool = False):
        """Display results for single audio analysis."""
        dominant_emotion, confidence = max(emotion_scores.items(), key=lambda x: x[1])
        dominant_emotion_jp = self._translate_emotion(dominant_emotion)
        
        # Main result panel
        result_text = f"[bold red]{dominant_emotion_jp}[/bold red]\n"
        result_text += f"[yellow]ä¿¡é ¼åº¦: {confidence:.2%}[/yellow]"
        
        panel = Panel(
            result_text,
            title="[bold green]ğŸ¯ ä¸»è¦ãªæ„Ÿæƒ…[/bold green]",
            border_style="green"
        )
        self.console.print(panel)
        
        if detailed:
            self._show_detailed_scores(emotion_scores)
    
    def show_segment_results(self, results: List[Tuple[float, float, Dict[str, float]]], 
                           metrics_results: List[Tuple[float, float, Dict[str, float]]] = None,
                           transcription_results: List[Tuple[float, float, Dict[str, str]]] = None):
        """Display results for segmented audio analysis."""
        if metrics_results and transcription_results:
            # Show combined emotion, audio metrics, and transcription table
            table = Table(title="ğŸ­ æ„Ÿæƒ…ãƒ»éŸ³å£°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ»æ–‡å­—èµ·ã“ã—ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³")
            table.add_column("æ™‚é–“", style="cyan", no_wrap=True)
            table.add_column("ãƒ†ã‚­ã‚¹ãƒˆ", style="bright_white", no_wrap=True, max_width=15)
            table.add_column("æ„Ÿæƒ…", style="red", no_wrap=True)
            table.add_column("ä¿¡é ¼åº¦", style="green", no_wrap=True)
            table.add_column("ãƒ”ãƒƒãƒ", style="yellow", no_wrap=True)
            table.add_column("æ˜åº¦", style="blue", no_wrap=True)
            table.add_column("éŸ³é‡", style="white", no_wrap=True)
            
            for i, ((start_time, end_time, emotion_scores), (_, _, audio_metrics), (_, _, transcription)) in enumerate(zip(results, metrics_results, transcription_results)):
                dominant_emotion, confidence = max(emotion_scores.items(), key=lambda x: x[1])
                dominant_emotion_jp = self._translate_emotion(dominant_emotion)
                
                table.add_row(
                    f"{start_time:.1f}s",
                    transcription.get('preview_text', '[ç©º]'),
                    dominant_emotion_jp,
                    f"{confidence:.0%}",
                    f"{audio_metrics.get('pitch_mean', 0):.0f}Hz",
                    f"{audio_metrics.get('brightness_score', 0):.0f}",
                    f"{audio_metrics.get('rms_mean', 0):.3f}"
                )
        elif transcription_results:
            # Show emotion and transcription table
            table = Table(title="ğŸ­ æ„Ÿæƒ…ãƒ»æ–‡å­—èµ·ã“ã—ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³")
            table.add_column("æ™‚é–“", style="cyan", no_wrap=True)
            table.add_column("é•·ã•", style="magenta")
            table.add_column("ãƒ†ã‚­ã‚¹ãƒˆ", style="bright_white", no_wrap=True, max_width=20)
            table.add_column("æ„Ÿæƒ…", style="red", no_wrap=True)
            table.add_column("ä¿¡é ¼åº¦", style="green")
            
            for i, ((start_time, end_time, emotion_scores), (_, _, transcription)) in enumerate(zip(results, transcription_results)):
                dominant_emotion, confidence = max(emotion_scores.items(), key=lambda x: x[1])
                dominant_emotion_jp = self._translate_emotion(dominant_emotion)
                duration = end_time - start_time
                
                table.add_row(
                    f"{start_time:.1f}s",
                    f"{duration:.1f}s",
                    transcription.get('preview_text', '[ç©º]'),
                    dominant_emotion_jp,
                    f"{confidence:.1%}"
                )
        elif metrics_results:
            # Show combined emotion and audio metrics table
            table = Table(title="ğŸ­ æ„Ÿæƒ…ãƒ»éŸ³å£°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³")
            table.add_column("æ™‚é–“", style="cyan", no_wrap=True)
            table.add_column("æ„Ÿæƒ…", style="red", no_wrap=True)
            table.add_column("ä¿¡é ¼åº¦", style="green", no_wrap=True)
            table.add_column("ãƒ”ãƒƒãƒ", style="yellow", no_wrap=True)
            table.add_column("æ˜åº¦", style="blue", no_wrap=True)
            table.add_column("æ¸…æ¶¼åº¦", style="magenta", no_wrap=True)
            table.add_column("éŸ³é‡", style="white", no_wrap=True)
            
            for i, ((start_time, end_time, emotion_scores), (_, _, audio_metrics)) in enumerate(zip(results, metrics_results)):
                dominant_emotion, confidence = max(emotion_scores.items(), key=lambda x: x[1])
                dominant_emotion_jp = self._translate_emotion(dominant_emotion)
                
                table.add_row(
                    f"{start_time:.1f}s",
                    dominant_emotion_jp,
                    f"{confidence:.0%}",
                    f"{audio_metrics.get('pitch_mean', 0):.0f}Hz",
                    f"{audio_metrics.get('brightness_score', 0):.0f}",
                    f"{audio_metrics.get('clarity_score', 0):.1f}",
                    f"{audio_metrics.get('rms_mean', 0):.3f}"
                )
        else:
            # Show emotion-only table
            table = Table(title="ğŸ­ æ„Ÿæƒ…åˆ†æã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³")
            table.add_column("æ™‚é–“", style="cyan", no_wrap=True)
            table.add_column("é•·ã•", style="magenta")
            table.add_column("æ„Ÿæƒ…", style="red", no_wrap=True)
            table.add_column("ä¿¡é ¼åº¦", style="green")
            
            for start_time, end_time, emotion_scores in results:
                dominant_emotion, confidence = max(emotion_scores.items(), key=lambda x: x[1])
                dominant_emotion_jp = self._translate_emotion(dominant_emotion)
                duration = end_time - start_time
                
                table.add_row(
                    f"{start_time:.1f}s",
                    f"{duration:.1f}s",
                    dominant_emotion_jp,
                    f"{confidence:.1%}"
                )
        
        self.console.print(table)
        
        # Show summary statistics
        self._show_summary_stats(results, metrics_results, transcription_results)
    
    def _show_detailed_scores(self, emotion_scores: Dict[str, float]):
        """Show detailed emotion scores table."""
        table = Table(title="ğŸ“Š è©³ç´°æ„Ÿæƒ…ã‚¹ã‚³ã‚¢")
        table.add_column("æ„Ÿæƒ…", style="cyan")
        table.add_column("ã‚¹ã‚³ã‚¢", style="green")
        table.add_column("ãƒãƒ¼", style="blue")
        
        for emotion, score in sorted(emotion_scores.items(), key=lambda x: x[1], reverse=True):
            emotion_jp = self._translate_emotion(emotion)
            bar_length = int(score * 20)  # Scale to 20 chars
            bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
            table.add_row(
                emotion_jp,
                f"{score:.2%}",
                bar
            )
        
        self.console.print(table)
    
    def _show_summary_stats(self, results: List[Tuple[float, float, Dict[str, float]]], 
                          metrics_results: List[Tuple[float, float, Dict[str, float]]] = None,
                          transcription_results: List[Tuple[float, float, Dict[str, str]]] = None):
        """Show summary statistics for segment analysis."""
        if not results:
            return
        
        # Calculate emotion distribution
        emotion_counts = {}
        total_duration = 0
        
        for start_time, end_time, emotion_scores in results:
            dominant_emotion, _ = max(emotion_scores.items(), key=lambda x: x[1])
            emotion_counts[dominant_emotion] = emotion_counts.get(dominant_emotion, 0) + 1
            total_duration += end_time - start_time
        
        # Create summary table
        table = Table(title="ğŸ“ˆ çµ±è¨ˆã‚µãƒãƒªãƒ¼")
        table.add_column("é …ç›®", style="cyan")
        table.add_column("å€¤", style="green")
        
        table.add_row("ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°", str(len(results)))
        table.add_row("ç·æ™‚é–“", f"{total_duration:.1f}ç§’")
        table.add_row("å¹³å‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·", f"{total_duration/len(results):.1f}ç§’")
        
        # Most common emotion
        most_common = max(emotion_counts.items(), key=lambda x: x[1])
        most_common_jp = self._translate_emotion(most_common[0])
        table.add_row("æœ€é »æ„Ÿæƒ…", f"{most_common_jp} ({most_common[1]} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ)")
        
        # Add audio metrics summary if available
        if metrics_results:
            all_pitch = [metrics.get('pitch_mean', 0) for _, _, metrics in metrics_results if metrics.get('pitch_mean', 0) > 0]
            all_brightness = [metrics.get('brightness_score', 0) for _, _, metrics in metrics_results]
            all_clarity = [metrics.get('clarity_score', 0) for _, _, metrics in metrics_results]
            all_volume = [metrics.get('rms_mean', 0) for _, _, metrics in metrics_results]
            
            if all_pitch:
                table.add_row("å¹³å‡ãƒ”ãƒƒãƒ", f"{np.mean(all_pitch):.0f} Hz")
            if all_brightness:
                table.add_row("å¹³å‡æ˜åº¦", f"{np.mean(all_brightness):.1f}")
            if all_clarity:
                table.add_row("å¹³å‡æ¸…æ¶¼åº¦", f"{np.mean(all_clarity):.1f}")
            if all_volume:
                table.add_row("å¹³å‡éŸ³é‡", f"{np.mean(all_volume):.3f}")
        
        # Add transcription summary if available
        if transcription_results:
            languages = [t.get('language', 'unknown') for _, _, t in transcription_results]
            most_common_lang = max(set(languages), key=languages.count) if languages else 'unknown'
            
            # Count segments with successful transcription
            successful_transcriptions = sum(1 for _, _, t in transcription_results if t.get('full_text', '').strip())
            transcription_rate = successful_transcriptions / len(transcription_results) * 100 if transcription_results else 0
            
            table.add_row("æ¤œå‡ºè¨€èª", most_common_lang.upper())
            table.add_row("è»¢å†™æˆåŠŸç‡", f"{transcription_rate:.1f}%")
        
        self.console.print(table)
    
    def show_audio_metrics_details(self, metrics_results: List[Tuple[float, float, Dict[str, float]]]):
        """Show detailed audio metrics analysis."""
        if not metrics_results:
            return
        
        # Pitch analysis table
        pitch_table = Table(title="ğŸµ ãƒ”ãƒƒãƒåˆ†æ")
        pitch_table.add_column("æ™‚é–“", style="cyan")
        pitch_table.add_column("å¹³å‡ãƒ”ãƒƒãƒ", style="yellow")
        pitch_table.add_column("ãƒ”ãƒƒãƒç¯„å›²", style="magenta")
        pitch_table.add_column("æœ‰éŸ³ç‡", style="green")
        
        for start_time, end_time, metrics in metrics_results:
            pitch_table.add_row(
                f"{start_time:.1f}s",
                f"{metrics.get('pitch_mean', 0):.0f} Hz",
                f"{metrics.get('pitch_range', 0):.0f} Hz",
                f"{metrics.get('voiced_ratio', 0):.1%}"
            )
        
        self.console.print(pitch_table)
        
        # Spectral analysis table
        spectral_table = Table(title="ğŸŒˆ ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†æ")
        spectral_table.add_column("æ™‚é–“", style="cyan")
        spectral_table.add_column("æ˜åº¦", style="blue")
        spectral_table.add_column("æ¸…æ¶¼åº¦", style="magenta")
        spectral_table.add_column("å¸¯åŸŸå¹…", style="yellow")
        
        for start_time, end_time, metrics in metrics_results:
            spectral_table.add_row(
                f"{start_time:.1f}s",
                f"{metrics.get('brightness_score', 0):.1f}",
                f"{metrics.get('clarity_score', 0):.1f}",
                f"{metrics.get('spectral_bandwidth_mean', 0):.0f} Hz"
            )
        
        self.console.print(spectral_table)